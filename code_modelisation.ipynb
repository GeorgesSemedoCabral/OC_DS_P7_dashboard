{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implémentez un modèle de scoring</h2>\n",
    "\n",
    "<h3>Première partie : modélisation</h3>\n",
    "\n",
    "<h4>I) Préparation de l\"environnement de travail</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"outputs.txt\", \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"{} - done in {:.0f} s\".format(title, time.time() - t0), file=file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>II) Importation et prétraitement des données</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding for categorical columns with get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns\n",
    "                           if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns,\n",
    "                        dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess application_train.csv and application_test.csv :\n",
    " - Read data and merge\n",
    " - Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    " - Categorical features with Binary encode (0 or 1; two categories)\n",
    " - Categorical features with One-Hot encode\n",
    " - NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    " - Some simple new features (percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(\"../../input/application_train.csv\", nrows=num_rows)\n",
    "    test_df = pd.read_csv(\"../../input/application_test.csv\", nrows=num_rows)\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)),\n",
    "          file=file)\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "    \n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    file.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess bureau.csv and bureau_balance.csv :\n",
    " - read and one-hot encode\n",
    " - Bureau balance: Perform aggregations and merge with bureau.csv\n",
    " - Bureau and bureau_balance numeric features\n",
    " - Bureau and bureau_balance categorical features\n",
    " - new dataframe with aggregations ?\n",
    " - Bureau: Active credits - using only numerical aggregations\n",
    " - Bureau: Closed credits - using only numerical aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(\"../../input/bureau.csv\", nrows=num_rows)\n",
    "    bb = pd.read_csv(\"../../input/bureau_balance.csv\", nrows=num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper()\n",
    "                               for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat:\n",
    "        cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg({**num_aggregations, \n",
    "                                                   **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index([\"BURO_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\"ACTIVE_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\"CLOSED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    \n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess previous_applications.csv :\n",
    " - read and one-hot encode\n",
    " - Days 365.243 values -> nan\n",
    " - Add feature: value ask / value received percentage\n",
    " - Previous applications numeric features\n",
    " - Previous applications categorical features\n",
    " - new dataframe with aggregations ?\n",
    " - Previous Applications: Approved Applications - only numerical features\n",
    " - Previous Applications: Refused Applications - only numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(\"../../input/previous_application.csv\", nrows=num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "    \n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\"PREV_\" + e[0] + \"_\" + e[1].upper() \n",
    "    for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\"APPROVED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                     for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\"REFUSED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                    for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess POS_CASH_balance.csv :\n",
    " - read and one-hot encode\n",
    " - Features\n",
    " - new dataframe with aggregations ?\n",
    " - Count pos cash accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(\"../../input/POS_CASH_balance.csv\", nrows=num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "    \n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\"POS_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess installments_payments.csv :\n",
    " - read and one-hot encode\n",
    " - Percentage and difference paid in each installment (amount paid and installment value)\n",
    " - Days past due and days before due (no negative values)\n",
    " - Features: Perform aggregations\n",
    " - new dataframe with aggregations ?\n",
    " - Count installments accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(\"../../input/installments_payments.csv\",\n",
    "                      nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\"INSTAL_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess credit_card_balance.csv :\n",
    " - read and one-hot encode\n",
    " - General aggregations\n",
    " - Count credit card lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(\"../../input/credit_card_balance.csv\", nrows=num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\"CC_\" + e[0] + \"_\" + e[1].upper()\n",
    "                               for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    \n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>III) Prédictions et représentations graphiques</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display/plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df_):\n",
    "    cols = (feature_importance_df_[[\"feature\", \"importance\"]]\n",
    "            .groupby(\"feature\").mean()\n",
    "            .sort_values(by=\"importance\", ascending=False)[:40].index)\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature\n",
    "                                               .isin(cols)]\n",
    "    data=best_features.sort_values(by=\"importance\", ascending=False)\n",
    "    fig = go.Figure()\n",
    "    fig.add_bar(x=data[\"importance\"], y=data[\"feature\"])\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        template=\"simple_white\",\n",
    "        showlegend=False,\n",
    "        title_text=\"LightGBM Features (avg over folds)\"\n",
    "    )\n",
    "    fig.write_image(\"lgbm_importances01.png\", engine=\"kaleido\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM, Random Forests, Logistic Regression and Dummy classifiers, with K-Folds or Stratified K-Folds :\n",
    " - Divide in training/validation and test data\n",
    " - (LOG only) imputation and feature scaling\n",
    " - print data shapes\n",
    " - Cross validation model\n",
    " - Create arrays and dataframes to store results\n",
    " - ranges of searching for models hyperparameters\n",
    " - call xs and ys,\n",
    " - models hyperparameters found by Grid Search\n",
    " - fit and predict\n",
    " - create dataframe for features importance\n",
    " - print fold and full AUC scores\n",
    " - Write submission file and plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model(df, num_folds, model, csv_name,\n",
    "                stratified=False, debug=False):\n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "\n",
    "    if model == \"LOG\" or model == \"RFC\":\n",
    "        train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        train_feats = list(train_df.columns)\n",
    "        train_imputer = SimpleImputer(strategy=\"median\")\n",
    "        train_imputer.fit(train_df)\n",
    "        train_df = train_imputer.transform(train_df)\n",
    "        train_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        train_scaler.fit(train_df)\n",
    "        train_df = train_scaler.transform(train_df)\n",
    "        train_df = pd.DataFrame(train_df, columns=train_feats)\n",
    "\n",
    "        test_df = test_df.drop(columns=\"TARGET\")\n",
    "        test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        test_feats = list(test_df.columns)\n",
    "        test_imputer = SimpleImputer(strategy=\"median\")\n",
    "        test_imputer.fit(test_df)\n",
    "        test_df = test_imputer.transform(test_df)\n",
    "        test_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        test_scaler.fit(test_df)\n",
    "        test_df = test_scaler.transform(test_df)\n",
    "        test_df = pd.DataFrame(test_df, columns=test_feats)\n",
    "\n",
    "    file=open(\"outputs.txt\", \"a\")\n",
    "    print(\"Starting {}. Train shape: {}, test shape: {}\"\n",
    "          .format(model, train_df.shape, test_df.shape), file=file)\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\", \"SK_ID_CURR\",\n",
    "             \"SK_ID_BUREAU\", \"SK_ID_PREV\", \"index\"]]\n",
    "\n",
    "    threads = range(1, 6)\n",
    "    estims = [10, 100, 1000, 10000, 100000, 1000000]\n",
    "    rates = np.logspace(-6, 0, 7)\n",
    "    leaves = range(10, 55, 5)\n",
    "    colsamples = range(0, 1, 100)\n",
    "    subsamples = range(0, 1, 100)\n",
    "    depths = range(5, 11)\n",
    "    alphas = np.logspace(-3, 8, 60)\n",
    "    lambdas = np.logspace(-3, 2, 6)\n",
    "    gains = np.logspace(-3, 0, 50)\n",
    "    weights = np.logspace(1, 2, 50)\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "        folds.split(train_df[feats], train_df[\"TARGET\"]\n",
    "    )):\n",
    "\n",
    "        train_x, train_y = (train_df[feats].iloc[train_idx],\n",
    "                            train_df[\"TARGET\"].iloc[train_idx])\n",
    "        valid_x, valid_y = (train_df[feats].iloc[valid_idx],\n",
    "                            train_df[\"TARGET\"].iloc[valid_idx])\n",
    "\n",
    "        if model == \"LGBM\":\n",
    "            param01 = {\n",
    "                \"nthread\": threads,\n",
    "                \"n_estimators\": estims,\n",
    "                \"learning_rate\": rates,\n",
    "                \"num_leaves\": leaves,\n",
    "                \"colsample_bytree\": colsamples,\n",
    "                \"subsample\": subsamples,\n",
    "                \"max_depth\": depths,\n",
    "                \"reg_alpha\": alphas,\n",
    "                \"reg_lambda\": lambdas,\n",
    "                \"min_split_gain\": gains,\n",
    "                \"min_child_weight\": weights\n",
    "            }\n",
    "            model_cv = GridSearchCV(\n",
    "                LGBMClassifier(\n",
    "                    silent=-1,\n",
    "                    verbose=-1\n",
    "                ),\n",
    "                param01,\n",
    "                scoring=\"accuracy\",\n",
    "                cv=10,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            best_cv = model_cv.fit(train_x,train_y)\n",
    "            best_params = best_cv.best_params_\n",
    "            clf = LGBMClassifier(\n",
    "                nthread=best_params[\"nthread\"],\n",
    "                n_estimators=best_params[\"n_estimators\"],\n",
    "                learning_rate=best_params[\"learning_rate\"],\n",
    "                num_leaves=best_params[\"num_leaves\"],\n",
    "                colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "                subsample=best_params[\"subsample\"],\n",
    "                max_depth=best_params[\"max_depth\"],\n",
    "                reg_alpha=best_params[\"reg_alpha\"],\n",
    "                reg_lambda=best_params[\"reg_lambda\"],\n",
    "                min_split_gain=best_params[\"min_split_gain\"],\n",
    "                min_child_weight=best_params[\"min_child_weight\"],\n",
    "                silent=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "        elif model == \"RFC\":\n",
    "            param02 = {\"n_estimators\": estims}\n",
    "            \"\"\"model_cv = GridSearchCV(\n",
    "                RandomForestClassifier(\n",
    "                    oob_score=True,\n",
    "                    random_state=50,\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1\n",
    "                ),\n",
    "                param02,\n",
    "                scoring = \"accuracy\",\n",
    "                cv = 10,\n",
    "                n_jobs = -1\n",
    "            )\n",
    "            best_cv = model_cv.fit(train_x,train_y)\n",
    "            best_n = best_cv.best_params_[\"n_estimators\"]\"\"\"\n",
    "            clf = RandomForestClassifier(\n",
    "                #n_estimators=best_n,\n",
    "                n_estimators=100,\n",
    "                oob_score=True,\n",
    "                random_state=50,\n",
    "                verbose=1,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        elif model == \"LOG\":\n",
    "            clf = LogisticRegression(\n",
    "                #Cs=9,\n",
    "                C=0.0001,\n",
    "                solver=\"saga\",\n",
    "                max_iter=1000,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "            #print(\"Meilleur hyperparamètre :\", clf.C_, file=file)\n",
    "        elif model == \"DUMMY\":\n",
    "            clf = DummyClassifier(\n",
    "                strategy=\"uniform\",  \n",
    "                random_state=42)\n",
    "\n",
    "        if model == \"DUMMY\" or model == \"LOG\":\n",
    "            clf.fit(\n",
    "                train_x,\n",
    "                train_y\n",
    "            )\n",
    "            oof_preds[valid_idx] = clf.predict_proba(\n",
    "                valid_x\n",
    "            )[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats]\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = np.zeros_like(feats)\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                               fold_importance_df], axis=0)\n",
    "        elif model == \"RFC\":\n",
    "            clf.fit(\n",
    "                train_x,\n",
    "                train_y\n",
    "            )\n",
    "            oof_preds[valid_idx] = clf.predict_proba(\n",
    "                valid_x\n",
    "            )[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats]\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                               fold_importance_df], axis=0)\n",
    "        elif model == \"LGBM\":\n",
    "            clf.fit(\n",
    "                train_x,\n",
    "                train_y,\n",
    "                eval_set=[\n",
    "                    (train_x, train_y),\n",
    "                    (valid_x, valid_y)\n",
    "                ],\n",
    "                eval_metric=\"auc\",\n",
    "                verbose=200,\n",
    "                early_stopping_rounds=200\n",
    "            )\n",
    "            oof_preds[valid_idx] = clf.predict_proba(\n",
    "                valid_x,\n",
    "                num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats],\n",
    "                num_iteration=clf.best_iteration_\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                               fold_importance_df], axis=0)\n",
    "\n",
    "        print(\"Fold %2d AUC : %.6f\" % (n_fold + 1, roc_auc_score(valid_y,\n",
    "              oof_preds[valid_idx])), file=file)\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"],\n",
    "          oof_preds), file=file)\n",
    "    file.close()\n",
    "\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(csv_name, index=False)\n",
    "\n",
    "    if model == \"RFC\" or model == \"LGBM\":\n",
    "        display_importances(feature_importance_df)\n",
    "        return feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>IV) Exécution du code</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(debug=False):\n",
    "    num_rows = 100000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Bureau df shape:\", bureau.shape, file=file)\n",
    "        df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Previous applications df shape:\", prev.shape, file=file)\n",
    "        df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del prev\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape, file=file)\n",
    "        df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del pos\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Installments payments df shape:\", ins.shape, file=file)\n",
    "        df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del ins\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Credit card balance df shape:\", cc.shape, file=file)\n",
    "        df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        df = df.rename(columns = lambda x: re.sub(\"[^A-Za-z0-9_]+\", \"\", x))\n",
    "        del cc\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Run dummies with k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"DUMMY\", \"dummies_kf.csv\",\n",
    "                                      stratified=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run dummies with stratified k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"DUMMY\", \"dummies_stratkf.csv\",\n",
    "                                      stratified=True, debug=debug)\n",
    "\n",
    "    with timer(\"Run logistic regression with k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"LOG\", \"logistic_kf.csv\",\n",
    "                                      stratified=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run logistic regression with stratified k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"LOG\", \"logistic_stratkf.csv\",\n",
    "                                      stratified=True, debug=debug)\n",
    "\n",
    "    with timer(\"Run Random Forests with k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"RFC\", \"rfc_kf.csv\",\n",
    "                                      stratified=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run Random Forests with stratified k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"RFC\", \"rfc_stratkf.csv\",\n",
    "                                      stratified=True, debug=debug)\n",
    "\n",
    "    \"\"\"with timer(\"Run LightGBM with k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"LGBM\", \"lightgbm_kf.csv\",\n",
    "                                      stratified=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run LightGBM with stratified k-folds\"):\n",
    "        feat_importance = kfold_model(df, 10, \"LGBM\", \"lightgbm_stratkf.csv\",\n",
    "                                      stratified=True, debug=debug)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd0924d7f569f28e2a54c0a26319272c996ac35e33779daa7927e118f16f5f21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
