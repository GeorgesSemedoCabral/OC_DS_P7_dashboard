{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implémentez un modèle de scoring</h2>\n",
    "\n",
    "<h3>Première partie : modélisation</h3>\n",
    "\n",
    "<h4>I) Préparation de l\"environnement de travail</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "import shap\n",
    "from bayes_opt import BayesianOptimization\n",
    "from collections import Counter\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from itertools import cycle\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    fbeta_score,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open(\"outputs.txt\", \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"{} - done in {:.0f} s\".format(title, time.time() - t0), file=file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>II) Importation et prétraitement des données</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding for categorical columns with get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns\n",
    "                           if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns,\n",
    "                        dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess application_train.csv and application_test.csv :\n",
    " - Read data and merge\n",
    " - Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    " - Categorical features with Binary encode (0 or 1; two categories)\n",
    " - Categorical features with One-Hot encode\n",
    " - NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    " - Some simple new features (percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(\"../../input/application_train.csv\", nrows=num_rows)\n",
    "    test_df = pd.read_csv(\"../../input/application_test.csv\", nrows=num_rows)\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)),\n",
    "          file=file)\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "    \n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    file.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess bureau.csv and bureau_balance.csv :\n",
    " - read and one-hot encode\n",
    " - Bureau balance: Perform aggregations and merge with bureau.csv\n",
    " - Bureau and bureau_balance numeric features\n",
    " - Bureau and bureau_balance categorical features\n",
    " - new dataframe with aggregations ?\n",
    " - Bureau: Active credits - using only numerical aggregations\n",
    " - Bureau: Closed credits - using only numerical aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(\"../../input/bureau.csv\", nrows=num_rows)\n",
    "    bb = pd.read_csv(\"../../input/bureau_balance.csv\", nrows=num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper()\n",
    "                               for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat:\n",
    "        cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg({**num_aggregations, \n",
    "                                                   **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index([\"BURO_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\"ACTIVE_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\"CLOSED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    \n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess previous_applications.csv :\n",
    " - read and one-hot encode\n",
    " - Days 365.243 values -> nan\n",
    " - Add feature: value ask / value received percentage\n",
    " - Previous applications numeric features\n",
    " - Previous applications categorical features\n",
    " - new dataframe with aggregations ?\n",
    " - Previous Applications: Approved Applications - only numerical features\n",
    " - Previous Applications: Refused Applications - only numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(\"../../input/previous_application.csv\", nrows=num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "    \n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\"PREV_\" + e[0] + \"_\" + e[1].upper() \n",
    "    for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\"APPROVED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                     for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\"REFUSED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                    for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess POS_CASH_balance.csv :\n",
    " - read and one-hot encode\n",
    " - Features\n",
    " - new dataframe with aggregations ?\n",
    " - Count pos cash accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(\"../../input/POS_CASH_balance.csv\", nrows=num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "    \n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\"POS_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess installments_payments.csv :\n",
    " - read and one-hot encode\n",
    " - Percentage and difference paid in each installment (amount paid and installment value)\n",
    " - Days past due and days before due (no negative values)\n",
    " - Features: Perform aggregations\n",
    " - new dataframe with aggregations ?\n",
    " - Count installments accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(\"../../input/installments_payments.csv\",\n",
    "                      nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\"INSTAL_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess credit_card_balance.csv :\n",
    " - read and one-hot encode\n",
    " - General aggregations\n",
    " - Count credit card lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(\"../../input/credit_card_balance.csv\", nrows=num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\"CC_\" + e[0] + \"_\" + e[1].upper()\n",
    "                               for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    \n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the whole dataset :\n",
    " - Divide in training/validation and test data\n",
    " - imputation and feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_impute_scale(df_):\n",
    "    train_df = df_[df_[\"TARGET\"].notnull()]\n",
    "    test_df = df_[df_[\"TARGET\"].isnull()]\n",
    "\n",
    "    train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    train_feats = list(train_df.columns)\n",
    "    train_imputer = SimpleImputer(strategy=\"median\")\n",
    "    train_imputer.fit(train_df)\n",
    "    train_df = train_imputer.transform(train_df)\n",
    "    train_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    train_scaler.fit(train_df)\n",
    "    train_df = train_scaler.transform(train_df)\n",
    "    train_df = pd.DataFrame(train_df, columns=train_feats)\n",
    "\n",
    "    test_df = test_df.drop(columns=\"TARGET\")\n",
    "    test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    test_feats = list(test_df.columns)\n",
    "    test_imputer = SimpleImputer(strategy=\"median\")\n",
    "    test_imputer.fit(test_df)\n",
    "    test_df = test_imputer.transform(test_df)\n",
    "    test_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    test_scaler.fit(test_df)\n",
    "    test_df = test_scaler.transform(test_df)\n",
    "    test_df = pd.DataFrame(test_df, columns=test_feats)\n",
    "\n",
    "    del df_\n",
    "    gc.collect()\n",
    "\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>III) Prédictions et représentations graphiques</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling of the training set for faster model hyperparameter tuning :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_data(train_df_):\n",
    "    sample = train_df_.sample(frac=0.1, random_state=42, axis=0,\n",
    "                              ignore_index=True)\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning models hyperparameters on the sampled training set, for each model :\n",
    " - define the function and the score to be optimized,\n",
    " - apply bayesian optimization to fine-tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_log(C, train_df_):\n",
    "    feats = [f for f in train_df_.columns if f not in [\"TARGET\", \"SK_ID_CURR\",\n",
    "             \"SK_ID_BUREAU\", \"SK_ID_PREV\", \"index\"]]\n",
    "\n",
    "    estimator = LogisticRegression(C=C, solver=\"saga\", max_iter=1000,\n",
    "                                   n_jobs=-1, random_state=42)\n",
    "                                   \n",
    "    cval = cross_val_score(estimator, train_df_[feats], train_df_[\"TARGET\"],\n",
    "                           scoring=\"roc_auc\", cv=10, n_jobs=-1)\n",
    "\n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_log(train_df_):\n",
    "    def log_crossval(C):\n",
    "        return tuning_log(C=C, train_df_=train_df_)\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=log_crossval,\n",
    "        pbounds={\"C\": (0.0001, 10000)},\n",
    "        random_state=42\n",
    "    )\n",
    "    optimizer.maximize()\n",
    "\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"Meilleur hyperparamètre :\", optimizer.max, file=file)\n",
    "    file.close()\n",
    "\n",
    "    return optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_rfc(n_estimators, min_samples_split, train_df_):\n",
    "    feats = [f for f in train_df_.columns if f not in [\"TARGET\", \"SK_ID_CURR\",\n",
    "             \"SK_ID_BUREAU\", \"SK_ID_PREV\", \"index\"]]\n",
    "\n",
    "    estimator = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        min_samples_split=min_samples_split,\n",
    "        oob_score=True,\n",
    "        random_state=50,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "                                   \n",
    "    cval = cross_val_score(estimator, train_df_[feats], train_df_[\"TARGET\"],\n",
    "                           scoring=\"roc_auc\", cv=10, n_jobs=-1)\n",
    "\n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_rfc(train_df_):\n",
    "    def rfc_crossval(n_estimators, min_samples_split):\n",
    "        return tuning_rfc(\n",
    "            n_estimators=int(n_estimators),\n",
    "            min_samples_split=int(min_samples_split),\n",
    "            train_df_=train_df_\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=rfc_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (700, 900),\n",
    "            \"min_samples_split\": (50, 100)\n",
    "        },\n",
    "        random_state=42\n",
    "    )\n",
    "    optimizer.maximize()\n",
    "\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"Meilleurs hyperparamètres :\", optimizer.max, file=file)\n",
    "    file.close()\n",
    "\n",
    "    return optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_lgbm(n_estimators, learning_rate, num_leaves, colsample_bytree,\n",
    "                subsample, reg_alpha, reg_lambda, min_split_gain,\n",
    "                min_child_weight, train_df_):\n",
    "    feats = [f for f in train_df_.columns if f not in [\"TARGET\", \"SK_ID_CURR\",\n",
    "             \"SK_ID_BUREAU\", \"SK_ID_PREV\", \"index\"]]\n",
    "\n",
    "    estimator = make_pipeline(\n",
    "        SMOTE(random_state=42, n_jobs=1),\n",
    "        LGBMClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            subsample=subsample,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            min_split_gain=min_split_gain,\n",
    "            min_child_weight=min_child_weight,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    "                                   \n",
    "    cval = cross_val_score(estimator, train_df_[feats], train_df_[\"TARGET\"],\n",
    "                           scoring=\"recall\", cv=10, n_jobs=-1)\n",
    "\n",
    "    return cval.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lgbm(train_df_):\n",
    "    def lgbm_crossval(n_estimators, learning_rate, num_leaves,\n",
    "                      colsample_bytree, subsample, reg_alpha,\n",
    "                      reg_lambda, min_split_gain, min_child_weight):\n",
    "        return tuning_lgbm(\n",
    "            n_estimators=int(n_estimators),\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=int(num_leaves),\n",
    "            colsample_bytree=colsample_bytree,\n",
    "            subsample=subsample,\n",
    "            reg_alpha=reg_alpha,\n",
    "            reg_lambda=reg_lambda,\n",
    "            min_split_gain=min_split_gain,\n",
    "            min_child_weight=min_child_weight,\n",
    "            train_df_=train_df_\n",
    "        )\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=lgbm_crossval,\n",
    "        pbounds={\n",
    "            \"n_estimators\": (100, 1000),\n",
    "            \"learning_rate\": (0.001, 0.1),\n",
    "            \"num_leaves\": (32, 128),\n",
    "            \"colsample_bytree\": (0.01, 1),\n",
    "            \"subsample\": (0.01, 1),\n",
    "            \"reg_alpha\": (0.0001, 10000),\n",
    "            \"reg_lambda\": (0.0001, 10000),\n",
    "            \"min_split_gain\": (0, 0.1),\n",
    "            \"min_child_weight\": (0.001, 1000)\n",
    "        },\n",
    "        random_state=42\n",
    "    )\n",
    "    optimizer.maximize()\n",
    "\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"Meilleurs hyperparamètres :\", optimizer.max, file=file)\n",
    "    file.close()\n",
    "\n",
    "    return optimizer.max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversampling the \"TARGET = 1\" category AFTER cross-validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_oversampling(x, y):\n",
    "    smote = SMOTE(sampling_strategy=\"minority\", random_state=42, n_jobs=-1)\n",
    "    x_smote, y_smote = smote.fit_resample(x, y)\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"Original fold shape :\", Counter(y), file=file)\n",
    "    print(\"Resampled fold shape :\", Counter(y_smote), file=file)\n",
    "    gc.collect()\n",
    "    file.close()\n",
    "    \n",
    "    return x_smote, y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df_, neg_values=False):\n",
    "    if neg_values:\n",
    "        cols = (feature_importance_df_[[\"feature\", \"abs_importance\"]]\n",
    "                .groupby(\"feature\")\n",
    "                .mean()\n",
    "                .sort_values(by=\"abs_importance\", ascending=False)[:40]\n",
    "                .index)\n",
    "    else:\n",
    "        cols = (feature_importance_df_[[\"feature\", \"importance\"]]\n",
    "                .groupby(\"feature\")\n",
    "                .mean()\n",
    "                .sort_values(by=\"importance\", ascending=False)[:40]\n",
    "                .index)\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature\n",
    "                                               .isin(cols)]\n",
    "    if neg_values:\n",
    "        data = best_features.sort_values(by=\"abs_importance\", ascending=True)\n",
    "    else:\n",
    "        data = best_features.sort_values(by=\"importance\", ascending=True)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    palette = cycle(px.colors.sequential.Turbo_r)\n",
    "    for feat in data[\"feature\"].unique():\n",
    "        feat_bar = data[data[\"feature\"]==feat]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=[feat_bar[\"importance\"].mean()],\n",
    "                y=[feat],\n",
    "                orientation=\"h\",\n",
    "                error_x=dict(\n",
    "                    type=\"constant\",\n",
    "                    value=feat_bar[\"importance\"].std()\n",
    "                ),\n",
    "                marker_color=next(palette)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1200,\n",
    "        width=1200,\n",
    "        template=\"simple_white\",\n",
    "        showlegend=False,\n",
    "        title_text=\"Feature Importance (avg over folds)\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_features(train_, labels_, feats_, test_, clf_):\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data=train_.values,\n",
    "        mode=\"classification\",\n",
    "        training_labels=labels_,\n",
    "        feature_names=feats_,\n",
    "        class_names=list(labels_.unique()),\n",
    "        random_state=42\n",
    "    )\n",
    "    explanation = explainer.explain_instance(test_.values[5], clf_)\n",
    "    explanation.show_in_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_features(clf_, train_):\n",
    "    explainer = shap.TreeExplainer(clf_)\n",
    "    shap_values = explainer.shap_values(train_)\n",
    "    shap.summary_plot(shap_values, train_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display classification metrics :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_metrics(train_df_, preds_, oof_preds_, model):\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=2,\n",
    "        subplot_titles=(\n",
    "            \"F-beta score as f(beta)\",\n",
    "            \"Precision-recall curve\",\n",
    "            \"F-beta score as f(beta, threshold)\",\n",
    "            \"Metrics as f(threshold)\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    betas = list(range(0, 16))\n",
    "    pre_model = []\n",
    "    rec_model = []\n",
    "    fbeta_model = []\n",
    "\n",
    "    for i in betas:\n",
    "        model_precision, model_recall, model_fbetascore, model_support = (\n",
    "            precision_recall_fscore_support(\n",
    "                train_df_[\"TARGET\"],\n",
    "                preds_,\n",
    "                beta=i,\n",
    "                average=\"binary\",\n",
    "                zero_division=0\n",
    "            )\n",
    "        )\n",
    "        pre_model.append(model_precision)\n",
    "        rec_model.append(model_recall)\n",
    "        fbeta_model.append(model_fbetascore)\n",
    "\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=betas,\n",
    "            y=pre_model,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group1\",\n",
    "            legendgrouptitle_text=\"F-beta score as f(beta)\",\n",
    "            name=\"Precision\",\n",
    "            line=dict(color=\"blue\")\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=betas,\n",
    "            y=rec_model,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group1\",\n",
    "            name=\"Recall\",\n",
    "            line=dict(color=\"red\")\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=betas,\n",
    "            y=fbeta_model,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group1\",\n",
    "            name=\"F-beta score\",\n",
    "            line=dict(color=\"green\")\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "    prcurve_y, prcurve_x, prcurve_t = precision_recall_curve(\n",
    "        train_df_[\"TARGET\"],\n",
    "        oof_preds_,\n",
    "        pos_label=1\n",
    "    )\n",
    "    ap_score = average_precision_score(\n",
    "        train_df_[\"TARGET\"],\n",
    "        oof_preds_,\n",
    "    )\n",
    "    ap_score = float(f\"{ap_score:.2f}\")\n",
    "\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=prcurve_x,\n",
    "            y=prcurve_y,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group2\",\n",
    "            legendgrouptitle_text=\"Precision-recall curve\",\n",
    "            name=\"{} (AP = {})\".format(model, ap_score)\n",
    "        ),\n",
    "        row=1,\n",
    "        col=2\n",
    "    )\n",
    "\n",
    "    def adjusted_metrics(y_preds, t):\n",
    "        return [1 if y >= t else 0 for y in y_preds]\n",
    "\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    betas_range = list(range(0, 6))\n",
    "    colors = [\"orchid\", \"royalblue\", \"limegreen\", \"gold\", \"orangered\",\n",
    "              \"darkred\"]\n",
    "\n",
    "    for i in betas_range:\n",
    "        fbeta_range = []\n",
    "        \n",
    "        for t in thresholds:\n",
    "            y_adj = adjusted_metrics(oof_preds_, t)\n",
    "            fbetascore = fbeta_score(train_df_[\"TARGET\"], y_adj, beta=i,\n",
    "                                     zero_division=0)\n",
    "            fbeta_range.append(fbetascore)\n",
    "\n",
    "        fig.append_trace(\n",
    "            go.Scatter(\n",
    "                x=thresholds,\n",
    "                y=fbeta_range,\n",
    "                mode=\"lines\",\n",
    "                legendgroup=\"group3\",\n",
    "                legendgrouptitle_text=\"F-beta score as f(beta, threshold)\",\n",
    "                name=\"beta = %2d\" % i,\n",
    "                line=dict(color=colors[i])\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1\n",
    "        )\n",
    "\n",
    "    pre_adj = []\n",
    "    rec_adj = []\n",
    "    fbeta_adj = []\n",
    "\n",
    "    for t in thresholds:\n",
    "        y_adj = adjusted_metrics(oof_preds_, t)\n",
    "        adj_precision, adj_recall, adj_fbetascore, adj_support = (\n",
    "            precision_recall_fscore_support(\n",
    "                train_df_[\"TARGET\"],\n",
    "                y_adj,\n",
    "                beta=1,\n",
    "                average=\"binary\",\n",
    "                zero_division=0\n",
    "            )\n",
    "        )\n",
    "        pre_adj.append(adj_precision)\n",
    "        rec_adj.append(adj_recall)\n",
    "        fbeta_adj.append(adj_fbetascore)\n",
    "\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=thresholds,\n",
    "            y=pre_adj,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group4\",\n",
    "            legendgrouptitle_text=\"Metrics as f(threshold)\",\n",
    "            name=\"Adjusted Precision\",\n",
    "            line=dict(color=\"blue\")\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2\n",
    "    )\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=thresholds,\n",
    "            y=rec_adj,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group4\",\n",
    "            name=\"Adjusted Recall\",\n",
    "            line=dict(color=\"red\")\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2\n",
    "    )\n",
    "    fig.append_trace(\n",
    "        go.Scatter(\n",
    "            x=thresholds,\n",
    "            y=fbeta_adj,\n",
    "            mode=\"lines\",\n",
    "            legendgroup=\"group4\",\n",
    "            name=\"Adjusted F1 score\",\n",
    "            line=dict(color=\"green\")\n",
    "        ),\n",
    "        row=2,\n",
    "        col=2\n",
    "    )   \n",
    "\n",
    "    fig.update_layout(height=800, width=1200, template=\"simple_white\",\n",
    "                      showlegend=True)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM, Random Forests, Logistic Regression and Dummy classifiers, with K-Folds or Stratified K-Folds, SMOTE-balanced or not :\n",
    " - print used model\n",
    " - Cross validation model\n",
    " - Create arrays and dataframes to store results\n",
    " - call xs and ys folds,\n",
    " - oversampling model,\n",
    " - fit and predict\n",
    " - create dataframe for features importance\n",
    " - print fold and full AUC scores\n",
    " - Write submission file and plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model(train_df, test_df, num_folds, model, filename, over=False,\n",
    "                strat=False, debug=False):\n",
    "    file = open(\"outputs.txt\", \"a\")\n",
    "    print(\"----------\", file=file)\n",
    "    print(\"Starting {}.\".format(model), file=file)\n",
    "    file.close()\n",
    "\n",
    "    if strat:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "    preds = np.zeros(train_df.shape[0])\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\", \"SK_ID_CURR\",\n",
    "             \"SK_ID_BUREAU\", \"SK_ID_PREV\", \"index\"]]\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "        folds.split(train_df[feats], train_df[\"TARGET\"]\n",
    "    )):\n",
    "        train_x, train_y = (train_df[feats].iloc[train_idx],\n",
    "                            train_df[\"TARGET\"].iloc[train_idx])\n",
    "        valid_x, valid_y = (train_df[feats].iloc[valid_idx],\n",
    "                            train_df[\"TARGET\"].iloc[valid_idx])\n",
    "\n",
    "        if over:\n",
    "            over_x, over_y = smote_oversampling(train_x, train_y)\n",
    "        else:\n",
    "            over_x, over_y = train_x, train_y\n",
    "\n",
    "        if model == \"LGBM\":\n",
    "            clf = LGBMClassifier(\n",
    "                objective=\"binary\",\n",
    "                n_estimators=int(291.10519961044855),\n",
    "                learning_rate=0.0030378649352844423,\n",
    "                num_leaves=int(49.45519685188166),\n",
    "                colsample_bytree=0.710991852018085,\n",
    "                subsample=0.5295088673159155,\n",
    "                reg_alpha=1834.0451801938873,\n",
    "                reg_lambda=3042.4224991711535,\n",
    "                min_split_gain=0.08324426408004218,\n",
    "                min_child_weight=969.9098822521422,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            clf.fit(\n",
    "                over_x,\n",
    "                over_y,\n",
    "                eval_set=[(over_x, over_y), (valid_x, valid_y)],\n",
    "                eval_metric=\"auc\",\n",
    "                callbacks=[\n",
    "                    log_evaluation(period=200),\n",
    "                    early_stopping(stopping_rounds=200)\n",
    "                ],\n",
    "            )\n",
    "            preds[valid_idx] = clf.predict(valid_x, \n",
    "                                           num_iteration=clf.best_iteration_)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(\n",
    "                valid_x,\n",
    "                num_iteration=clf.best_iteration_\n",
    "            )[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats],\n",
    "                num_iteration=clf.best_iteration_\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                              fold_importance_df], axis=0)\n",
    "        elif model == \"RFC\":\n",
    "            clf = RandomForestClassifier(\n",
    "                n_estimators=600,\n",
    "                min_samples_split=25,\n",
    "                oob_score=True,\n",
    "                random_state=50,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            clf.fit(over_x, over_y)\n",
    "            preds[valid_idx] = clf.predict(valid_x)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats]\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                              fold_importance_df], axis=0)\n",
    "        elif model == \"LOG\":\n",
    "            clf = LogisticRegression(\n",
    "                C=0.3807947176588889,\n",
    "                solver=\"saga\",\n",
    "                max_iter=1000,\n",
    "                n_jobs=-1,\n",
    "                random_state=42\n",
    "            )\n",
    "            clf.fit(over_x, over_y)\n",
    "            preds[valid_idx] = clf.predict(valid_x)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats]\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = clf.coef_[0]\n",
    "            fold_importance_df[\"abs_importance\"] = np.absolute(clf.coef_[0])\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                              fold_importance_df], axis=0)\n",
    "        elif model == \"DUMMY\":\n",
    "            clf = DummyClassifier(strategy=\"uniform\", random_state=42)\n",
    "            clf.fit(over_x, over_y)\n",
    "            preds[valid_idx] = clf.predict(valid_x)\n",
    "            oof_preds[valid_idx] = clf.predict_proba(valid_x)[:, 1]\n",
    "            sub_preds += clf.predict_proba(\n",
    "                test_df[feats]\n",
    "            )[:, 1] / folds.n_splits\n",
    "            fold_importance_df = pd.DataFrame()\n",
    "            fold_importance_df[\"feature\"] = feats\n",
    "            fold_importance_df[\"importance\"] = np.zeros_like(feats)\n",
    "            fold_importance_df[\"fold\"] = n_fold + 1\n",
    "            feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                              fold_importance_df], axis=0)\n",
    "\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Fold %2d AUC : %.6f\" % (n_fold + 1, roc_auc_score(valid_y,\n",
    "              oof_preds[valid_idx])), file=file)\n",
    "        fold_precision, fold_recall, fold_fbetascore, fold_support = (\n",
    "            precision_recall_fscore_support(valid_y, preds[valid_idx], beta=1, \n",
    "                                            average=\"binary\", zero_division=0)\n",
    "        )\n",
    "        print(\" - precision :\", fold_precision, file=file)\n",
    "        print(\" - recall :\", fold_recall, file=file)\n",
    "        print(\" - F1 score :\", fold_fbetascore, file=file)\n",
    "\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"],\n",
    "          oof_preds), file=file)\n",
    "    file.close()\n",
    "\n",
    "    class_metrics(train_df, preds, oof_preds, model)\n",
    "\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(filename, index=False)\n",
    "\n",
    "    if model == \"LOG\":\n",
    "        display_importances(feature_importance_df, neg_values=True)\n",
    "    elif model == \"RFC\" or model == \"LGBM\":\n",
    "        display_importances(feature_importance_df, neg_values=False)\n",
    "        shap_features(clf, over_x)\n",
    "        lime_features(over_x, over_y, feats, test_df[feats], clf.predict_proba)\n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>IV) Exécution du code</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(debug=False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Bureau df shape:\", bureau.shape, file=file)\n",
    "        df = df.join(bureau, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Previous applications df shape:\", prev.shape, file=file)\n",
    "        df = df.join(prev, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del prev\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape, file=file)\n",
    "        df = df.join(pos, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del pos\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Installments payments df shape:\", ins.shape, file=file)\n",
    "        df = df.join(ins, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        del ins\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Credit card balance df shape:\", cc.shape, file=file)\n",
    "        df = df.join(cc, how=\"left\", on=\"SK_ID_CURR\")\n",
    "        df = df.rename(columns = lambda x: re.sub(\"[^A-Za-z0-9_]+\", \"\", x))\n",
    "        del cc\n",
    "        gc.collect()\n",
    "        file.close()\n",
    "\n",
    "    with timer(\"Split, impute and scale data\"):\n",
    "        train_df, test_df = split_impute_scale(df)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Train shape: {}, test shape: {}\"\n",
    "              .format(train_df.shape, test_df.shape), file=file)\n",
    "        file.close()\n",
    "\n",
    "    \"\"\"with timer(\"Sample training set\"):\n",
    "        sample_df = sampling_data(train_df)\n",
    "        file = open(\"outputs.txt\", \"a\")\n",
    "        print(\"Sampled train shape :\", sample_df.shape, file=file)\n",
    "        file.close()\"\"\"\n",
    "\n",
    "    \"\"\"with timer(\"Bayesian optimization of logistic regression hyperparameter\"):\n",
    "        best_c = optimize_log(sample_df)\"\"\"\n",
    "\n",
    "    \"\"\"with timer(\"Bayesian optimization of Random Forests hyperparameters\"):\n",
    "        best_n = optimize_rfc(sample_df)\"\"\"\n",
    "\n",
    "    \"\"\"with timer(\"Bayesian optimization of LightGBM hyperparameters\"):\n",
    "        best_params = optimize_lgbm(train_df)\"\"\"\n",
    "        \n",
    "    \"\"\"with timer(\"Run dummies with k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"DUMMY\",\n",
    "            \"dum_kf.csv\", over=False, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run dummies with k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"DUMMY\",\n",
    "            \"dum_kf_bal.csv\", over=True, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run dummies with stratified k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"DUMMY\",\n",
    "            \"dum_strat.csv\", over=False, strat=True, debug=debug)\n",
    "\n",
    "    with timer(\"Run dummies with stratified k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"DUMMY\",\n",
    "            \"dum_strat_bal.csv\", over=True, strat=True, debug=debug)\"\"\"\n",
    "\n",
    "    \"\"\"with timer(\"Run logistic regression with k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LOG\",\n",
    "            \"log_kf.csv\", over=False, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run logistic regression with k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LOG\",\n",
    "            \"log_kf_bal.csv\", over=True, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run logistic regression with stratified k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LOG\",\n",
    "            \"log_strat.csv\", over=False, strat=True, debug=debug)\n",
    "\n",
    "    with timer(\"Run logistic regression with stratified k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LOG\",\n",
    "            \"log_strat_bal.csv\", over=True, strat=True, debug=debug)\"\"\"\n",
    "\n",
    "    \"\"\"with timer(\"Run Random Forests with k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"RFC\",\n",
    "            \"rfc_kf.csv\", over=False, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run Random Forests with k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"RFC\",\n",
    "            \"rfc_kf_bal.csv\", over=True, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run Random Forests with stratified k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"RFC\",\n",
    "            \"rfc_strat.csv\", over=False, strat=True, debug=debug)\n",
    "\n",
    "    with timer(\"Run Random Forests with stratified k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"RFC\",\n",
    "            \"rfc_strat_bal.csv\", over=True, strat=True, debug=debug)\"\"\"\n",
    "\n",
    "    with timer(\"Run LightGBM with k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LGBM\",\n",
    "            \"lgbm_kf.csv\", over=False, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run LightGBM with k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LGBM\",\n",
    "            \"lgbm_kf_bal.csv\", over=True, strat=False, debug=debug)\n",
    "\n",
    "    with timer(\"Run LightGBM with stratified k-folds, unbalanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LGBM\",\n",
    "            \"lgbm_strat.csv\", over=False, strat=True, debug=debug)\n",
    "\n",
    "    with timer(\"Run LightGBM with stratified k-folds, balanced\"):\n",
    "        feat_importance = kfold_model(train_df, test_df, 10, \"LGBM\",\n",
    "            \"lgbm_strat_bal.csv\", over=True, strat=True, debug=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd0924d7f569f28e2a54c0a26319272c996ac35e33779daa7927e118f16f5f21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
