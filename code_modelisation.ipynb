{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Implémentez un modèle de scoring</h2>\n",
    "\n",
    "<h3>Première partie : modélisation</h3>\n",
    "\n",
    "<h4>I) Préparation de l\"environnement de travail</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.io as pio\n",
    "pio.renderers.default=\"notebook\"\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode()\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{}- done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>II) Importation et prétraitement des données</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding for categorical columns with get_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoder(df, nan_as_category=True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns\n",
    "                           if df[col].dtype == \"object\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_columns,\n",
    "                        dummy_na=nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    \n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess application_train.csv and application_test.csv :\n",
    " - Read data and merge\n",
    " - Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    " - Categorical features with Binary encode (0 or 1; two categories)\n",
    " - Categorical features with One-Hot encode\n",
    " - NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    " - Some simple new features (percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def application_train_test(num_rows=None, nan_as_category=False):\n",
    "    df = pd.read_csv(\"../../input/application_train.csv\", nrows=num_rows)\n",
    "    test_df = pd.read_csv(\"../../input/application_test.csv\", nrows=num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "\n",
    "    df = df[df[\"CODE_GENDER\"] != \"XNA\"]\n",
    "    \n",
    "    for bin_feature in [\"CODE_GENDER\", \"FLAG_OWN_CAR\", \"FLAG_OWN_REALTY\"]:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    df[\"DAYS_EMPLOYED\"].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    df[\"DAYS_EMPLOYED_PERC\"] = df[\"DAYS_EMPLOYED\"] / df[\"DAYS_BIRTH\"]\n",
    "    df[\"INCOME_CREDIT_PERC\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"AMT_CREDIT\"]\n",
    "    df[\"INCOME_PER_PERSON\"] = df[\"AMT_INCOME_TOTAL\"] / df[\"CNT_FAM_MEMBERS\"]\n",
    "    df[\"ANNUITY_INCOME_PERC\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_INCOME_TOTAL\"]\n",
    "    df[\"PAYMENT_RATE\"] = df[\"AMT_ANNUITY\"] / df[\"AMT_CREDIT\"]\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess bureau.csv and bureau_balance.csv :\n",
    " - read and one-hot encode\n",
    " - Bureau balance: Perform aggregations and merge with bureau.csv\n",
    " - Bureau and bureau_balance numeric features\n",
    " - Bureau and bureau_balance categorical features\n",
    " - new dataframe with aggregations ?\n",
    " - Bureau: Active credits - using only numerical aggregations\n",
    " - Bureau: Closed credits - using only numerical aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bureau_and_balance(num_rows=None, nan_as_category=True):\n",
    "    bureau = pd.read_csv(\"../../input/bureau.csv\", nrows=num_rows)\n",
    "    bb = pd.read_csv(\"../../input/bureau_balance.csv\", nrows=num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    bb_aggregations = {\"MONTHS_BALANCE\": [\"min\", \"max\", \"size\"]}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = [\"mean\"]\n",
    "    bb_agg = bb.groupby(\"SK_ID_BUREAU\").agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper()\n",
    "                               for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how=\"left\", on=\"SK_ID_BUREAU\")\n",
    "    bureau.drop([\"SK_ID_BUREAU\"], axis=1, inplace=True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"DAYS_CREDIT\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"DAYS_CREDIT_ENDDATE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_CREDIT_UPDATE\": [\"mean\"],\n",
    "        \"CREDIT_DAY_OVERDUE\": [\"max\", \"mean\"],\n",
    "        \"AMT_CREDIT_MAX_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_DEBT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_CREDIT_SUM_OVERDUE\": [\"mean\"],\n",
    "        \"AMT_CREDIT_SUM_LIMIT\": [\"mean\", \"sum\"],\n",
    "        \"AMT_ANNUITY\": [\"max\", \"mean\"],\n",
    "        \"CNT_CREDIT_PROLONG\": [\"sum\"],\n",
    "        \"MONTHS_BALANCE_MIN\": [\"min\"],\n",
    "        \"MONTHS_BALANCE_MAX\": [\"max\"],\n",
    "        \"MONTHS_BALANCE_SIZE\": [\"mean\", \"sum\"]\n",
    "    }\n",
    "\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "    for cat in bb_cat:\n",
    "        cat_aggregations[cat + \"_MEAN\"] = [\"mean\"]\n",
    "\n",
    "    bureau_agg = bureau.groupby(\"SK_ID_CURR\").agg({**num_aggregations, \n",
    "                                                   **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index([\"BURO_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in bureau_agg.columns.tolist()])\n",
    "\n",
    "    active = bureau[bureau[\"CREDIT_ACTIVE_Active\"] == 1]\n",
    "    active_agg = active.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index([\"ACTIVE_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "\n",
    "    closed = bureau[bureau[\"CREDIT_ACTIVE_Closed\"] == 1]\n",
    "    closed_agg = closed.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index([\"CLOSED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                   for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    \n",
    "    return bureau_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess previous_applications.csv :\n",
    " - read and one-hot encode\n",
    " - Days 365.243 values -> nan\n",
    " - Add feature: value ask / value received percentage\n",
    " - Previous applications numeric features\n",
    " - Previous applications categorical features\n",
    " - new dataframe with aggregations ?\n",
    " - Previous Applications: Approved Applications - only numerical features\n",
    " - Previous Applications: Refused Applications - only numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_applications(num_rows=None, nan_as_category=True):\n",
    "    prev = pd.read_csv(\"../../input/previous_application.csv\", nrows=num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category=True)\n",
    "\n",
    "    prev[\"DAYS_FIRST_DRAWING\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_FIRST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE_1ST_VERSION\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_LAST_DUE\"].replace(365243, np.nan, inplace=True)\n",
    "    prev[\"DAYS_TERMINATION\"].replace(365243, np.nan, inplace=True)\n",
    "\n",
    "    prev[\"APP_CREDIT_PERC\"] = prev[\"AMT_APPLICATION\"] / prev[\"AMT_CREDIT\"]\n",
    "\n",
    "    num_aggregations = {\n",
    "        \"AMT_ANNUITY\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_APPLICATION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_CREDIT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"APP_CREDIT_PERC\": [\"min\", \"max\", \"mean\", \"var\"],\n",
    "        \"AMT_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"AMT_GOODS_PRICE\": [\"min\", \"max\", \"mean\"],\n",
    "        \"HOUR_APPR_PROCESS_START\": [\"min\", \"max\", \"mean\"],\n",
    "        \"RATE_DOWN_PAYMENT\": [\"min\", \"max\", \"mean\"],\n",
    "        \"DAYS_DECISION\": [\"min\", \"max\", \"mean\"],\n",
    "        \"CNT_PAYMENT\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = [\"mean\"]\n",
    "    \n",
    "    prev_agg = prev.groupby(\"SK_ID_CURR\").agg({**num_aggregations,\n",
    "                                               **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index([\"PREV_\" + e[0] + \"_\" + e[1].upper() \n",
    "    for e in prev_agg.columns.tolist()])\n",
    "\n",
    "    approved = prev[prev[\"NAME_CONTRACT_STATUS_Approved\"] == 1]\n",
    "    approved_agg = approved.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index([\"APPROVED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                     for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "    refused = prev[prev[\"NAME_CONTRACT_STATUS_Refused\"] == 1]\n",
    "    refused_agg = refused.groupby(\"SK_ID_CURR\").agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index([\"REFUSED_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                    for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how=\"left\", on=\"SK_ID_CURR\")\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "\n",
    "    return prev_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess POS_CASH_balance.csv :\n",
    " - read and one-hot encode\n",
    " - Features\n",
    " - new dataframe with aggregations ?\n",
    " - Count pos cash accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_cash(num_rows=None, nan_as_category=True):\n",
    "    pos = pd.read_csv(\"../../input/POS_CASH_balance.csv\", nrows=num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category=True)\n",
    "\n",
    "    aggregations = {\n",
    "        \"MONTHS_BALANCE\": [\"max\", \"mean\", \"size\"],\n",
    "        \"SK_DPD\": [\"max\", \"mean\"],\n",
    "        \"SK_DPD_DEF\": [\"max\", \"mean\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "    \n",
    "    pos_agg = pos.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    pos_agg.columns = pd.Index([\"POS_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                for e in pos_agg.columns.tolist()])\n",
    "\n",
    "    pos_agg[\"POS_COUNT\"] = pos.groupby(\"SK_ID_CURR\").size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "\n",
    "    return pos_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess installments_payments.csv :\n",
    " - read and one-hot encode\n",
    " - Percentage and difference paid in each installment (amount paid and installment value)\n",
    " - Days past due and days before due (no negative values)\n",
    " - Features: Perform aggregations\n",
    " - new dataframe with aggregations ?\n",
    " - Count installments accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def installments_payments(num_rows=None, nan_as_category=True):\n",
    "    ins = pd.read_csv(\"../../input/installments_payments.csv\",\n",
    "                      nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category=True)\n",
    "\n",
    "    ins[\"PAYMENT_PERC\"] = ins[\"AMT_PAYMENT\"] / ins[\"AMT_INSTALMENT\"]\n",
    "    ins[\"PAYMENT_DIFF\"] = ins[\"AMT_INSTALMENT\"] - ins[\"AMT_PAYMENT\"]\n",
    "\n",
    "    ins[\"DPD\"] = ins[\"DAYS_ENTRY_PAYMENT\"] - ins[\"DAYS_INSTALMENT\"]\n",
    "    ins[\"DBD\"] = ins[\"DAYS_INSTALMENT\"] - ins[\"DAYS_ENTRY_PAYMENT\"]\n",
    "    ins[\"DPD\"] = ins[\"DPD\"].apply(lambda x: x if x > 0 else 0)\n",
    "    ins[\"DBD\"] = ins[\"DBD\"].apply(lambda x: x if x > 0 else 0)\n",
    "\n",
    "    aggregations = {\n",
    "        \"NUM_INSTALMENT_VERSION\": [\"nunique\"],\n",
    "        \"DPD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"DBD\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"PAYMENT_PERC\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"PAYMENT_DIFF\": [\"max\", \"mean\", \"sum\", \"var\"],\n",
    "        \"AMT_INSTALMENT\": [\"max\", \"mean\", \"sum\"],\n",
    "        \"AMT_PAYMENT\": [\"min\", \"max\", \"mean\", \"sum\"],\n",
    "        \"DAYS_ENTRY_PAYMENT\": [\"max\", \"mean\", \"sum\"]\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = [\"mean\"]\n",
    "\n",
    "    ins_agg = ins.groupby(\"SK_ID_CURR\").agg(aggregations)\n",
    "    ins_agg.columns = pd.Index([\"INSTAL_\" + e[0] + \"_\" + e[1].upper()\n",
    "                                for e in ins_agg.columns.tolist()])\n",
    "\n",
    "    ins_agg[\"INSTAL_COUNT\"] = ins.groupby(\"SK_ID_CURR\").size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "\n",
    "    return ins_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess credit_card_balance.csv :\n",
    " - read and one-hot encode\n",
    " - General aggregations\n",
    " - Count credit card lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def credit_card_balance(num_rows=None, nan_as_category=True):\n",
    "    cc = pd.read_csv(\"../../input/credit_card_balance.csv\", nrows=num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category=True)\n",
    "\n",
    "    cc.drop([\"SK_ID_PREV\"], axis=1, inplace=True)\n",
    "    cc_agg = cc.groupby(\"SK_ID_CURR\").agg([\"min\", \"max\", \"mean\", \"sum\", \"var\"])\n",
    "    cc_agg.columns = pd.Index([\"CC_\" + e[0] + \"_\" + e[1].upper()\n",
    "                               for e in cc_agg.columns.tolist()])\n",
    "\n",
    "    cc_agg[\"CC_COUNT\"] = cc.groupby(\"SK_ID_CURR\").size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    \n",
    "    return cc_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>III) Prédictions et représentations graphiques</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models with KFold or Stratified KFold :\n",
    " - Divide in training/validation and test data\n",
    " - Cross validation model\n",
    " - Create arrays and dataframes to store results\n",
    " - call xs and ys,\n",
    " - models hyperparameters found by Bayesian optimization\n",
    " - fit and predict\n",
    " - create dataframe for features importance\n",
    " - print fold and full AUC scores\n",
    " - Write submission file and plot feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_model(df, model, num_folds, stratified=False, debug=False):    \n",
    "    train_df = df[df[\"TARGET\"].notnull()]\n",
    "    test_df = df[df[\"TARGET\"].isnull()]\n",
    "    print(\n",
    "        \"Starting {}. Train shape: {}, test shape: {}\".format(\n",
    "            model, train_df.shape, test_df.shape\n",
    "        )\n",
    "    )\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits=num_folds, shuffle=True,\n",
    "                                random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits=num_folds, shuffle=True, random_state=1001)\n",
    "\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in [\"TARGET\",\"SK_ID_CURR\",\n",
    "             \"SK_ID_BUREAU\",\"SK_ID_PREV\",\"index\"]]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(\n",
    "        folds.split(train_df[feats], train_df[\"TARGET\"]\n",
    "    )):\n",
    "        train_x, train_y = (train_df[feats].iloc[train_idx],\n",
    "                            train_df[\"TARGET\"].iloc[train_idx])\n",
    "        valid_x, valid_y = (train_df[feats].iloc[valid_idx],\n",
    "                            train_df[\"TARGET\"].iloc[valid_idx])\n",
    "\n",
    "        if model == \"LightGBM\":\n",
    "            clf = LGBMClassifier(\n",
    "                nthread=4,\n",
    "                n_estimators=10000,\n",
    "                learning_rate=0.02,\n",
    "                num_leaves=34,\n",
    "                colsample_bytree=0.9497036,\n",
    "                subsample=0.8715623,\n",
    "                max_depth=8,\n",
    "                reg_alpha=0.041545473,\n",
    "                reg_lambda=0.0735294,\n",
    "                min_split_gain=0.0222415,\n",
    "                min_child_weight=39.3259775,\n",
    "                silent=-1,\n",
    "                verbose=-1\n",
    "            )\n",
    "\n",
    "        clf.fit(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            eval_set=[\n",
    "                (train_x, train_y),\n",
    "                (valid_x, valid_y)\n",
    "            ],\n",
    "            eval_metric=\"auc\",\n",
    "            verbose=200,\n",
    "            early_stopping_rounds=200\n",
    "        )\n",
    "        oof_preds[valid_idx] = clf.predict_proba(\n",
    "            valid_x,\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1]\n",
    "        sub_preds += clf.predict_proba(\n",
    "            test_df[feats],\n",
    "            num_iteration=clf.best_iteration_\n",
    "        )[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df,\n",
    "                                          fold_importance_df], axis=0)\n",
    "\n",
    "        print(\"Fold %2d AUC : %.6f\" % (n_fold + 1, roc_auc_score(valid_y,\n",
    "              oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Full AUC score %.6f\" % roc_auc_score(train_df[\"TARGET\"], oof_preds))\n",
    "\n",
    "    if not debug:\n",
    "        test_df[\"TARGET\"] = sub_preds\n",
    "        test_df[[\"SK_ID_CURR\", \"TARGET\"]].to_csv(\"submission_kernel02.csv\",\n",
    "                                                 index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "\n",
    "    return feature_importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bd0924d7f569f28e2a54c0a26319272c996ac35e33779daa7927e118f16f5f21"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
